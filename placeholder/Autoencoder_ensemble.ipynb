{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.7\n",
    "GRF_F_V_PRO_right = pd.read_csv(r'GRF_F_V_PRO_right.csv')\n",
    "patient_IDs = np.unique(GRF_F_V_PRO_right['SUBJECT_ID'])\n",
    "\n",
    "# Number of samples in the training data set\n",
    "train_size = int(len(patient_IDs)*train_fraction)\n",
    "\n",
    "patient_IDs_train, patient_IDs_test = train_test_split(patient_IDs, train_size = train_size, random_state=random_SEED)\n",
    "\n",
    "# Create test dataset\n",
    "train_set = GRF_F_V_PRO_right.loc[GRF_F_V_PRO_right['SUBJECT_ID'].isin(patient_IDs_train)]\n",
    "test_set = GRF_F_V_PRO_right.loc[GRF_F_V_PRO_right['SUBJECT_ID'].isin(patient_IDs_test)]\n",
    "\n",
    "train_set.index = pd.RangeIndex(len(train_set.index))\n",
    "test_set.index = pd.RangeIndex(len(test_set.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure train_set is a standalone DataFrame\n",
    "train_set = train_set.copy()\n",
    "\n",
    "# Subtract mean for numeric columns from column 3 onwards\n",
    "numeric_cols = train_set.iloc[:, 3:].select_dtypes(include='number')\n",
    "numeric_cols = numeric_cols - numeric_cols.mean()\n",
    "train_set.loc[:, train_set.columns[3:]] = numeric_cols\n",
    "\n",
    "# Ensure train_set is a standalone DataFrame\n",
    "test_set = test_set.copy()\n",
    "\n",
    "# Subtract mean for numeric columns from column 3 onwards\n",
    "numeric_cols = test_set.iloc[:, 3:].select_dtypes(include='number')\n",
    "numeric_cols = numeric_cols - numeric_cols.mean()\n",
    "test_set.loc[:, test_set.columns[3:]] = numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length = test_set.iloc[:,3:].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train sets into several trainsets for each autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(random_SEED)\n",
    "\n",
    "train_datasets = []\n",
    "\n",
    "# Create a copy of the IDs to sample from without replacement\n",
    "remaining_IDs = patient_IDs_train.copy()\n",
    "\n",
    "for i in range(num_splits):\n",
    "    if i == num_splits - 1:  # For the last split, take all remaining IDs\n",
    "        split = remaining_IDs\n",
    "    else:\n",
    "        split = np.random.choice(remaining_IDs, size=len(patient_IDs_train) // num_splits, replace=False)\n",
    "        # Remove sampled IDs from remaining IDs\n",
    "        remaining_IDs = np.setdiff1d(remaining_IDs, split)\n",
    "    \n",
    "    i_train_set = train_set.loc[train_set['SUBJECT_ID'].isin(split)]\n",
    "    i_train_set.index = pd.RangeIndex(len(i_train_set.index))\n",
    "    train_datasets.append(i_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Autoencoder Model\n",
    "class TimeSeriesAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(TimeSeriesAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, encoding_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, original_autoencoder):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Copy encoder layers from the trained autoencoder\n",
    "        self.encoder = original_autoencoder.encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_loader(split, train_datasets):\n",
    "    \n",
    "    train_data = train_datasets[split]\n",
    "    train_data = np.array(train_data,dtype=np.float32)[:,3:]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(train_data))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    return(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Autoencoder\n",
    "def training_loop(num_epochs, dataloader, model, optimizer, criterion):\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)  # MSE between output and input\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print average loss\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = sequence_length\n",
    "encoding_dim = 10  # Dimension of the encoded representation\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the ensamble of autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0063\n",
      "Epoch [1/20], Loss: 0.0069\n",
      "Epoch [1/20], Loss: 0.0066\n",
      "Epoch [1/20], Loss: 0.0067\n",
      "Epoch [1/20], Loss: 0.0070\n",
      "Epoch [1/20], Loss: 0.0078\n",
      "Epoch [1/20], Loss: 0.0073\n",
      "Epoch [1/20], Loss: 0.0075\n",
      "Epoch [1/20], Loss: 0.0083\n",
      "Epoch [1/20], Loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    model = TimeSeriesAutoencoder(input_dim=input_dim, encoding_dim=encoding_dim)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    dataloader = train_data_loader(i, train_datasets)\n",
    "    \n",
    "    model = training_loop(num_epochs, dataloader, model, optimizer, criterion)\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train one model on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesAutoencoder(input_dim=input_dim, encoding_dim=encoding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(np.array(train_set,dtype=np.float32)[:,3:]))\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = training_loop(num_epochs, dataloader, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.6096e-05)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Autoencoder on test data\n",
    "# Testing the model on a few samples to see reconstruction\n",
    "test_data = np.array(test_set, dtype=np.float32)[:,3:]\n",
    "test_data_torch = torch.tensor(test_data)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(test_data_torch)\n",
    "    mse = criterion(model(test_data_torch), test_data_torch)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "reconstructed_per_model = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1):\n",
    "        i_model = models[i]\n",
    "        i_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        reconstructed_per_model.append(i_model(test_data_torch).numpy())\n",
    "\n",
    "    reconstructed_mean = np.array(reconstructed_per_model).mean(axis=0)\n",
    "    mse_mean = criterion(torch.tensor(reconstructed_mean), test_data_torch)\n",
    "    print(mse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8))  # Create 2 subplots vertically stacked\n",
    "\n",
    "# First plot: Test Data vs Reconstructed\n",
    "for i, j in enumerate([100, 200, 300]):  # Plot three trajectories\n",
    "    axes[0].plot(test_data_torch[j], label=f'Test Data {i+1}', linestyle='-', color=colors[i])\n",
    "    axes[0].plot(reconstructed[j], label=f'Reconstructed {i+1}', linestyle='--', color=colors[i])\n",
    "\n",
    "axes[0].set_title(\"Comparison of Test and Reconstructed Data Trajectories\")\n",
    "axes[0].set_xlabel(\"Time Step\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].legend(loc='lower center', ncol=2)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Second plot: Test Data vs Reconstructed Mean\n",
    "for i, j in enumerate([100, 200, 300]):  # Plot three trajectories\n",
    "    axes[1].plot(test_data_torch[j], label=f'Test Data {i+1}', linestyle='-', color=colors[i])\n",
    "    axes[1].plot(reconstructed_mean[j], label=f'Reconstructed Mean {i+1}', linestyle='--', color=colors[i])\n",
    "\n",
    "axes[1].set_title(\"Comparison of Test and Reconstructed Mean Data Trajectories\")\n",
    "axes[1].set_xlabel(\"Time Step\")\n",
    "axes[1].set_ylabel(\"Value\")\n",
    "axes[1].legend(loc='lower center', ncol=2)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()  # Ensure the subplots do not overlap\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
